{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "k2xX1DPT21ZS",
        "outputId": "d4de17ee-617a-4895-9907-9168fb9354b0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[33mWARNING: Skipping optimum as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping optimum-intel as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.7/40.7 MB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m39.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
            "\u001b[0m  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m68.4/68.4 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m90.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m75.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m45.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.2/176.2 MB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m547.8/547.8 kB\u001b[0m \u001b[31m33.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m314.1/314.1 kB\u001b[0m \u001b[31m22.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.4/12.4 MB\u001b[0m \u001b[31m88.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.1/318.1 kB\u001b[0m \u001b[31m21.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.9/15.9 MB\u001b[0m \u001b[31m81.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.2/43.2 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.8/119.8 MB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m857.8/857.8 kB\u001b[0m \u001b[31m35.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.2/307.2 kB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m424.7/424.7 kB\u001b[0m \u001b[31m23.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.1/141.1 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.8/40.8 MB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.1/4.1 MB\u001b[0m \u001b[31m76.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m249.1/249.1 kB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.9/64.9 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.1/10.1 MB\u001b[0m \u001b[31m102.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.4/62.4 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.0/92.0 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.9/71.9 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.6/53.6 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.0/76.0 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m74.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.7/307.7 kB\u001b[0m \u001b[31m21.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.4/341.4 kB\u001b[0m \u001b[31m22.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m86.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m48.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for optimum-intel (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for nncf (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for transformers_stream_generator (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for jstyleson (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for ffmpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for grapheme (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "cudf-cu12 24.4.1 requires pyarrow<15.0.0a0,>=14.0.1, but you have pyarrow 16.1.0 which is incompatible.\n",
            "google-colab 1.0.0 requires requests==2.31.0, but you have requests 2.32.3 which is incompatible.\n",
            "ibis-framework 8.0.0 requires pyarrow<16,>=2, but you have pyarrow 16.1.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "%pip install -Uq pip\n",
        "%pip uninstall -q -y optimum optimum-intel\n",
        "%pip install --pre -Uq openvino openvino-tokenizers[transformers] --extra-index-url https://storage.openvinotoolkit.org/simple/wheels/nightly\n",
        "%pip install -q --extra-index-url https://download.pytorch.org/whl/cpu\\\n",
        "\"git+https://github.com/huggingface/optimum-intel.git\"\\\n",
        "\"git+https://github.com/openvinotoolkit/nncf.git\"\\\n",
        "\"torch>=2.1\"\\\n",
        "\"datasets\" \\\n",
        "\"accelerate\"\\\n",
        "\"gradio>=4.19\"\\\n",
        "\"onnx\" \"einops\" \"transformers_stream_generator\" \"tiktoken\" \"transformers>=4.40\" \"bitsandbytes\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login, whoami\n",
        "try:\n",
        "    whoami()\n",
        "    print('Authorization token already provided')\n",
        "except OSError:\n",
        "    notebook_login()"
      ],
      "metadata": {
        "id": "Ic5Uc2Eb24Db"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import ipywidgets as widgets"
      ],
      "metadata": {
        "id": "yjPrfyQt30_F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DEFAULT_SYSTEM_PROMPT = \"\"\"\\\n",
        "You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\n",
        "If a question does not make any sense or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\\\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "q6hhBx7o3_hq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_config = {\n",
        "                    \"model_id\": \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n",
        "                    \"remote_code\": False,\n",
        "                    \"start_message\": f\"<|system|>\\n{DEFAULT_SYSTEM_PROMPT}</s>\\n\",\n",
        "                    \"history_template\": \"<|user|>\\n{user}</s> \\n<|assistant|>\\n{assistant}</s> \\n\",\n",
        "                    \"current_message_template\": \"<|user|>\\n{user}</s> \\n<|assistant|>\\n{assistant}\"\n",
        "                }"
      ],
      "metadata": {
        "id": "tobmp9Gw3_87"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def llama_partial_text_processor(partial_text, new_text):\n",
        "    new_text = new_text.replace(\"[INST]\", \"\").replace(\"[/INST]\", \"\")\n",
        "    partial_text += new_text\n",
        "    return partial_text"
      ],
      "metadata": {
        "id": "T9FvG-qN4B3l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "\n",
        "pt_model_id = model_config[\"model_id\"]\n",
        "pt_model_name = pt_model_id.split(\"/\")[1]\n",
        "int4_model_dir = Path(pt_model_name) / \"INT4_compressed_weights\""
      ],
      "metadata": {
        "id": "xC6metSx4E5L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_to_int4():\n",
        "    model_compression_params = {\n",
        "        \"sym\": False,\n",
        "        \"group_size\": 128,\n",
        "        \"ratio\": 0.8,\n",
        "    }\n",
        "\n",
        "    if (int4_model_dir / \"openvino_model.xml\").exists():\n",
        "        return\n",
        "    remote_code = model_config.get(\"remote_code\", False)\n",
        "    export_command_base = \"optimum-cli export openvino --model {} --task text-generation-with-past --weight-format int4\".format(pt_model_id)\n",
        "    int4_compression_args = \" --group-size {} --ratio {}\".format(model_compression_params[\"group_size\"], model_compression_params[\"ratio\"])\n",
        "    if model_compression_params[\"sym\"]:\n",
        "        int4_compression_args += \" --sym\"\n",
        "    export_command_base += int4_compression_args\n",
        "    if remote_code:\n",
        "        export_command_base += \" --trust-remote-code\"\n",
        "    export_command = export_command_base + \" \" + str(int4_model_dir)\n",
        "    ! $export_command"
      ],
      "metadata": {
        "id": "4ZJ0C3z34HqZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "convert_to_int4()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c9W0eCVh5KZD",
        "outputId": "55a38e31-241d-457f-c273-f4fd92e4e0a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-07-13 07:00:51.547090: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-07-13 07:00:51.547143: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-07-13 07:00:51.715587: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-07-13 07:00:52.051101: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-07-13 07:00:55.033874: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "No CUDA runtime is found, using CUDA_HOME='/usr/local/cuda'\n",
            "config.json: 100% 608/608 [00:00<00:00, 2.42MB/s]\n",
            "Framework not specified. Using pt to export the model.\n",
            "model.safetensors: 100% 2.20G/2.20G [00:21<00:00, 103MB/s]\n",
            "generation_config.json: 100% 124/124 [00:00<00:00, 543kB/s]\n",
            "tokenizer_config.json: 100% 1.29k/1.29k [00:00<00:00, 6.41MB/s]\n",
            "tokenizer.model: 100% 500k/500k [00:00<00:00, 280MB/s]\n",
            "tokenizer.json: 100% 1.84M/1.84M [00:00<00:00, 1.91MB/s]\n",
            "special_tokens_map.json: 100% 551/551 [00:00<00:00, 3.04MB/s]\n",
            "Using framework PyTorch: 2.3.0+cu121\n",
            "Overriding 1 configuration item(s)\n",
            "\t- use_cache -> True\n",
            "/usr/local/lib/python3.10/dist-packages/optimum/exporters/openvino/model_patcher.py:467: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "  if sequence_length != 1:\n",
            "['input_ids', 'attention_mask', 'position_ids', 'past_key_values']\n",
            "\u001b[2KMixed-Precision assignment \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m100%\u001b[0m \u001b[36m154/154\u001b[0m • \u001b[36m0:00:18\u001b[0m • \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hINFO:nncf:Statistics of the bitwidth distribution:\n",
            "┍━━━━━━━━━━━━━━━━┯━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┯━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┑\n",
            "│   Num bits (N) │ % all parameters (layers)   │ % ratio-defining parameters (layers)   │\n",
            "┝━━━━━━━━━━━━━━━━┿━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┿━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┥\n",
            "│              8 │ 30% (42 / 156)              │ 20% (40 / 154)                         │\n",
            "├────────────────┼─────────────────────────────┼────────────────────────────────────────┤\n",
            "│              4 │ 70% (114 / 156)             │ 80% (114 / 154)                        │\n",
            "┕━━━━━━━━━━━━━━━━┷━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┷━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┙\n",
            "\u001b[2KApplying Weight Compression \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m100%\u001b[0m \u001b[36m156/156\u001b[0m • \u001b[36m0:00:45\u001b[0m • \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "int4_weights = int4_model_dir / \"openvino_model.bin\"\n",
        "\n",
        "if int4_weights.exists():\n",
        "    print(f\"Size of model with INT4 compressed weights is {int4_weights.stat().st_size / 1024 / 1024:.2f} MB\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PGPbCeJA5Nhe",
        "outputId": "15f20938-7a14-4f21-d28f-bb509fede1ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Size of model with INT4 compressed weights is 696.19 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import openvino as ov\n",
        "\n",
        "core = ov.Core()"
      ],
      "metadata": {
        "id": "klga4bKQ6VBi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoConfig, AutoTokenizer\n",
        "from optimum.intel.openvino import OVModelForCausalLM\n",
        "\n",
        "ov_config = {\"PERFORMANCE_HINT\": \"LATENCY\", \"NUM_STREAMS\": \"1\", \"CACHE_DIR\": \"\"}\n",
        "\n",
        "model_name = model_config[\"model_id\"]\n",
        "tok = AutoTokenizer.from_pretrained(int4_model_dir, trust_remote_code=True)\n",
        "\n",
        "ov_model = OVModelForCausalLM.from_pretrained(\n",
        "    int4_model_dir,\n",
        "    device = \"CPU\",\n",
        "    ov_config=ov_config,\n",
        "    config=AutoConfig.from_pretrained(int4_model_dir, trust_remote_code=True),\n",
        "    trust_remote_code=True,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A2Ma-W4h_U2Z",
        "outputId": "30e21db9-53d0-4e3d-c4b2-c5edca03a61b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No CUDA runtime is found, using CUDA_HOME='/usr/local/cuda'\n",
            "The argument `trust_remote_code` is to be used along with export=True. It will be ignored.\n",
            "Compiling the model to CPU ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_string = \"2 + 2 =\"\n",
        "input_tokens = tok(test_string, return_tensors=\"pt\")\n",
        "answer = ov_model.generate(**input_tokens, max_new_tokens=2)\n",
        "print(tok.batch_decode(answer, skip_special_tokens=True)[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pwnbyO78AhnU",
        "outputId": "225aaa78-4a18-4006-9cdd-5874a3cbb6d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2 + 2 = 4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from threading import Event, Thread\n",
        "from uuid import uuid4\n",
        "from typing import List, Tuple\n",
        "import gradio as gr\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    StoppingCriteria,\n",
        "    StoppingCriteriaList,\n",
        "    TextIteratorStreamer,\n",
        ")\n",
        "\n",
        "\n",
        "model_name = model_config[\"model_id\"]\n",
        "start_message = model_config[\"start_message\"]\n",
        "history_template = model_config.get(\"history_template\")\n",
        "current_message_template = model_config.get(\"current_message_template\")\n",
        "\n",
        "examples = [\n",
        "    [\"Hello there! How are you doing?\"],\n",
        "    [\"Tell me about yourself\"],\n",
        "    [\"Can you explain to me briefly what is Python programming language?\"],\n",
        "    [\"Explain the plot of Arcane.\"],\n",
        "    [\"What are some common mistakes to avoid when writing code?\"],\n",
        "    [\"Write a haiku about love.\"],\n",
        "    [\"How can I improve my english?\"]\n",
        "]\n",
        "\n",
        "max_new_tokens = 256\n",
        "\n",
        "\n",
        "def convert_history_to_token(history: List[Tuple[str, str]]):\n",
        "    \"\"\"\n",
        "    function for conversion history stored as list pairs of user and assistant messages to tokens according to model expected conversation template\n",
        "    Params:\n",
        "      history: dialogue history\n",
        "    Returns:\n",
        "      history in token format\n",
        "    \"\"\"\n",
        "    text = start_message + \"\".join(\n",
        "        [\"\".join([history_template.format(num=round, user=item[0], assistant=item[1])]) for round, item in enumerate(history[:-1])]\n",
        "    )\n",
        "    text += \"\".join(\n",
        "        [\n",
        "            \"\".join(\n",
        "                [\n",
        "                    current_message_template.format(\n",
        "                        num=len(history) + 1,\n",
        "                        user=history[-1][0],\n",
        "                        assistant=history[-1][1],\n",
        "                    )\n",
        "                ]\n",
        "            )\n",
        "        ]\n",
        "    )\n",
        "    # input_token = tok(text, return_tensors=\"pt\", **tokenizer_kwargs).input_ids\n",
        "    input_token = tok(text, return_tensors=\"pt\").input_ids\n",
        "    return input_token\n",
        "\n",
        "\n",
        "def user(message, history):\n",
        "    \"\"\"\n",
        "    callback function for updating user messages in interface on submit button click\n",
        "\n",
        "    Params:\n",
        "      message: current message\n",
        "      history: conversation history\n",
        "    Returns:\n",
        "      None\n",
        "    \"\"\"\n",
        "    # Append the user's message to the conversation history\n",
        "    return \"\", history + [[message, \"\"]]\n",
        "\n",
        "\n",
        "def bot(history, temperature, top_p, top_k, repetition_penalty, conversation_id):\n",
        "    \"\"\"\n",
        "    callback function for running chatbot on submit button click\n",
        "\n",
        "    Params:\n",
        "      history: conversation history\n",
        "      temperature:  parameter for control the level of creativity in AI-generated text.\n",
        "                    By adjusting the `temperature`, you can influence the AI model's probability distribution, making the text more focused or diverse.\n",
        "      top_p: parameter for control the range of tokens considered by the AI model based on their cumulative probability.\n",
        "      top_k: parameter for control the range of tokens considered by the AI model based on their cumulative probability, selecting number of tokens with highest probability.\n",
        "      repetition_penalty: parameter for penalizing tokens based on how frequently they occur in the text.\n",
        "      conversation_id: unique conversation identifier.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    # Construct the input message string for the model by concatenating the current system message and conversation history\n",
        "    # Tokenize the messages string\n",
        "    input_ids = convert_history_to_token(history)\n",
        "    if input_ids.shape[1] > 2000:\n",
        "        history = [history[-1]]\n",
        "        input_ids = convert_history_to_token(history)\n",
        "    streamer = TextIteratorStreamer(tok, timeout=30.0, skip_prompt=True, skip_special_tokens=True)\n",
        "    generate_kwargs = dict(\n",
        "        input_ids=input_ids,\n",
        "        max_new_tokens=max_new_tokens,\n",
        "        temperature=temperature,\n",
        "        do_sample=temperature > 0.0,\n",
        "        top_p=top_p,\n",
        "        top_k=top_k,\n",
        "        repetition_penalty=repetition_penalty,\n",
        "        streamer=streamer,\n",
        "    )\n",
        "\n",
        "    stream_complete = Event()\n",
        "\n",
        "    def generate_and_signal_complete():\n",
        "        \"\"\"\n",
        "        genration function for single thread\n",
        "        \"\"\"\n",
        "        global start_time\n",
        "        ov_model.generate(**generate_kwargs)\n",
        "        stream_complete.set()\n",
        "\n",
        "    t1 = Thread(target=generate_and_signal_complete)\n",
        "    t1.start()\n",
        "\n",
        "    # Initialize an empty string to store the generated text\n",
        "    partial_text = \"\"\n",
        "    for new_text in streamer:\n",
        "        partial_text = llama_partial_text_processor(partial_text, new_text)\n",
        "        history[-1][1] = partial_text\n",
        "        yield history\n",
        "\n",
        "\n",
        "def request_cancel():\n",
        "    ov_model.request.cancel()\n",
        "\n",
        "\n",
        "def get_uuid():\n",
        "    \"\"\"\n",
        "    universal unique identifier for thread\n",
        "    \"\"\"\n",
        "    return str(uuid4())"
      ],
      "metadata": {
        "id": "xMxKsfmuA4G-",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with gr.Blocks(\n",
        "    theme=gr.themes.Soft(),\n",
        "    css=\".disclaimer {font-variant-caps: all-small-caps;}\",\n",
        ") as demo:\n",
        "    conversation_id = gr.State(get_uuid)\n",
        "    gr.Markdown(f\"\"\"<h1><center>OpenVINO {model_name} Chatbot</center></h1>\"\"\")\n",
        "    chatbot = gr.Chatbot(height=500)\n",
        "    with gr.Row():\n",
        "        with gr.Column():\n",
        "            msg = gr.Textbox(\n",
        "                label=\"Chat Message Box\",\n",
        "                placeholder=\"Chat Message Box\",\n",
        "                show_label=False,\n",
        "                container=False,\n",
        "            )\n",
        "        with gr.Column():\n",
        "            with gr.Row():\n",
        "                submit = gr.Button(\"Submit\")\n",
        "                stop = gr.Button(\"Stop\")\n",
        "                clear = gr.Button(\"Clear\")\n",
        "    with gr.Row():\n",
        "        with gr.Accordion(\"Advanced Options:\", open=False):\n",
        "            with gr.Row():\n",
        "                with gr.Column():\n",
        "                    with gr.Row():\n",
        "                        temperature = gr.Slider(\n",
        "                            label=\"Temperature\",\n",
        "                            value=0.1,\n",
        "                            minimum=0.0,\n",
        "                            maximum=1.0,\n",
        "                            step=0.1,\n",
        "                            interactive=True,\n",
        "                            info=\"Higher values produce more diverse outputs\",\n",
        "                        )\n",
        "                with gr.Column():\n",
        "                    with gr.Row():\n",
        "                        top_p = gr.Slider(\n",
        "                            label=\"Top-p (nucleus sampling)\",\n",
        "                            value=1.0,\n",
        "                            minimum=0.0,\n",
        "                            maximum=1,\n",
        "                            step=0.01,\n",
        "                            interactive=True,\n",
        "                            info=(\n",
        "                                \"Sample from the smallest possible set of tokens whose cumulative probability \"\n",
        "                                \"exceeds top_p. Set to 1 to disable and sample from all tokens.\"\n",
        "                            ),\n",
        "                        )\n",
        "                with gr.Column():\n",
        "                    with gr.Row():\n",
        "                        top_k = gr.Slider(\n",
        "                            label=\"Top-k\",\n",
        "                            value=50,\n",
        "                            minimum=0.0,\n",
        "                            maximum=200,\n",
        "                            step=1,\n",
        "                            interactive=True,\n",
        "                            info=\"Sample from a shortlist of top-k tokens — 0 to disable and sample from all tokens.\",\n",
        "                        )\n",
        "                with gr.Column():\n",
        "                    with gr.Row():\n",
        "                        repetition_penalty = gr.Slider(\n",
        "                            label=\"Repetition Penalty\",\n",
        "                            value=1.1,\n",
        "                            minimum=1.0,\n",
        "                            maximum=2.0,\n",
        "                            step=0.1,\n",
        "                            interactive=True,\n",
        "                            info=\"Penalize repetition — 1.0 to disable.\",\n",
        "                        )\n",
        "    gr.Examples(examples, inputs=msg, label=\"Click on any example and press the 'Submit' button\")\n",
        "\n",
        "    submit_event = msg.submit(\n",
        "        fn=user,\n",
        "        inputs=[msg, chatbot],\n",
        "        outputs=[msg, chatbot],\n",
        "        queue=False,\n",
        "    ).then(\n",
        "        fn=bot,\n",
        "        inputs=[\n",
        "            chatbot,\n",
        "            temperature,\n",
        "            top_p,\n",
        "            top_k,\n",
        "            repetition_penalty,\n",
        "            conversation_id,\n",
        "        ],\n",
        "        outputs=chatbot,\n",
        "        queue=True,\n",
        "    )\n",
        "    submit_click_event = submit.click(\n",
        "        fn=user,\n",
        "        inputs=[msg, chatbot],\n",
        "        outputs=[msg, chatbot],\n",
        "        queue=False,\n",
        "    ).then(\n",
        "        fn=bot,\n",
        "        inputs=[\n",
        "            chatbot,\n",
        "            temperature,\n",
        "            top_p,\n",
        "            top_k,\n",
        "            repetition_penalty,\n",
        "            conversation_id,\n",
        "        ],\n",
        "        outputs=chatbot,\n",
        "        queue=True,\n",
        "    )\n",
        "    stop.click(\n",
        "        fn=request_cancel,\n",
        "        inputs=None,\n",
        "        outputs=None,\n",
        "        cancels=[submit_event, submit_click_event],\n",
        "        queue=False,\n",
        "    )\n",
        "    clear.click(lambda: None, None, chatbot, queue=False)\n",
        "\n",
        "demo.launch()"
      ],
      "metadata": {
        "id": "KBDBLIsatB88"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "# Load original model from Hugging Face\n",
        "original_model_id = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(original_model_id)\n",
        "original_model = AutoModelForCausalLM.from_pretrained(original_model_id)"
      ],
      "metadata": {
        "id": "AJ9H4Nj5dN97"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_history_to_token(history, tokenizer):\n",
        "    text = start_message + \"\".join(\n",
        "        [\"\".join([history_template.format(num=round, user=item[0], assistant=item[1])]) for round, item in enumerate(history[:-1])]\n",
        "    )\n",
        "    text += \"\".join(\n",
        "        [\n",
        "            \"\".join(\n",
        "                [\n",
        "                    current_message_template.format(\n",
        "                        num=len(history) + 1,\n",
        "                        user=history[-1][0],\n",
        "                        assistant=history[-1][1],\n",
        "                    )\n",
        "                ]\n",
        "            )\n",
        "        ]\n",
        "    )\n",
        "    input_token = tokenizer(text, return_tensors=\"pt\").input_ids\n",
        "    return input_token\n",
        "\n",
        "def generate_response(model, tokenizer, history, temperature=0.1, top_p=1.0, top_k=50, repetition_penalty=1.1):\n",
        "    input_ids = convert_history_to_token(history, tokenizer)\n",
        "    generate_kwargs = dict(\n",
        "        input_ids=input_ids,\n",
        "        max_new_tokens=max_new_tokens,\n",
        "        temperature=temperature,\n",
        "        do_sample=temperature > 0.0,\n",
        "        top_p=top_p,\n",
        "        top_k=top_k,\n",
        "        repetition_penalty=repetition_penalty,\n",
        "    )\n",
        "    output_ids = model.generate(**generate_kwargs)\n",
        "    response = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "    return response\n"
      ],
      "metadata": {
        "id": "2q9QUUMXdU9z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Iterate over examples\n",
        "total_time = 0\n",
        "for example in examples:\n",
        "    history = [[example[0], \"\"]]  # Initialize history with the current user input\n",
        "    start_time = time.time()\n",
        "    response = generate_response(original_model, tokenizer, history)\n",
        "    end_time = time.time()\n",
        "    total_time += end_time - start_time\n",
        "    print(f\"Input: {example[0]}\")\n",
        "    print(f\"Response: {response}\")\n",
        "    print()\n",
        "\n",
        "original_time = total_time/len(examples)\n",
        "\n",
        "total_time = 0\n",
        "for example in examples:\n",
        "    history = [[example[0], \"\"]]  # Initialize history with the current user input\n",
        "    start_time = time.time()\n",
        "    response = generate_response(ov_model, tokenizer, history)\n",
        "    end_time = time.time()\n",
        "    total_time += end_time - start_time\n",
        "    print(f\"Input: {example[0]}\")\n",
        "    print(f\"Response: {response}\")\n",
        "    print()\n",
        "\n",
        "openvino_time = total_time/len(examples)\n",
        "\n",
        "print(\"\\n\\n\")\n",
        "print(f\"Original model average inference time: {original_time:.2f} seconds\")\n",
        "print(f\"Openvino model average inference time: {openvino_time:.2f} seconds\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JLIOds7JdyTB",
        "outputId": "86ec9373-b7c7-4d8a-e48a-36a19d221752"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: Hello there! How are you doing?\n",
            "Response: <|system|>\n",
            "You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\n",
            "If a question does not make any sense or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information. \n",
            "<|user|>\n",
            "Hello there! How are you doing?  \n",
            "<|assistant|>\n",
            "I am doing well, thank you for asking. I hope you are too.\n",
            "\n",
            "i am glad to hear from you. I am doing fine, thanks for asking.\n",
            "\n",
            "i am good, thank you for asking. I am happy to hear from you.\n",
            "\n",
            "i am fine, thank you for asking. I am happy to hear from you.\n",
            "\n",
            "i am great, thank you for asking. I am happy to hear from you.\n",
            "\n",
            "i am having a great day, thank you for asking. I am happy to hear from you.\n",
            "\n",
            "i am having a wonderful day, thank you for asking. I am happy to hear from you.\n",
            "\n",
            "i am feeling great today, thank you for asking. I am happy to hear from you.\n",
            "\n",
            "i am feeling amazing today, thank you for asking. I am happy to hear from you.\n",
            "\n",
            "i am feeling very happy today, thank you for asking. I am happy to hear from you.\n",
            "\n",
            "i am feeling grateful today, thank you for asking. I am happy to hear from you.\n",
            "\n",
            "i am feeling blessed today, thank you for asking. I am happy to hear from you.\n",
            "\n",
            "i am feeling lucky today, thank you for\n",
            "\n",
            "Input: Tell me about yourself\n",
            "Response: <|system|>\n",
            "You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\n",
            "If a question does not make any sense or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information. \n",
            "<|user|>\n",
            "Tell me about yourself  \n",
            "<|assistant|>\n",
            "I am a friendly, curious, and creative individual who loves learning new things and exploring different cultures. I have a Bachelor's degree in psychology from a reputable university, and I have experience working with individuals with various mental health issues. In my free time, I enjoy reading, cooking, and spending time with friends and family. I am passionate about helping others and making a positive impact on the world around me.\n",
            "\n",
            "Input: Can you explain to me briefly what is Python programming language?\n",
            "Response: <|system|>\n",
            "You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\n",
            "If a question does not make any sense or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information. \n",
            "<|user|>\n",
            "Can you explain to me briefly what is Python programming language?  \n",
            "<|assistant|>\n",
            "Sure! Python is an interpreted, high-level, general-purpose programming language with dynamic typing. It was developed by Guido van Rossum at the University of Cambridge in the UK in 1989. Python has a simple syntax and a focus on readability and maintainability. It supports multiple data types, including strings, numbers, booleans, lists, dictionaries, tuples, and functions. Python also has built-in libraries for various tasks such as file handling, network communication, database access, and scientific computing. Python is widely used in various fields such as web development, artificial intelligence, machine learning, data science, and more.\n",
            "\n",
            "Input: Explain the plot of Arcane.\n",
            "Response: <|system|>\n",
            "You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\n",
            "If a question does not make any sense or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information. \n",
            "<|user|>\n",
            "Explain the plot of Arcane.  \n",
            "<|assistant|>\n",
            "Arcane is a fantasy novel by Brandon Sanderson, published in 2014. The story follows the journey of two young orphans, Lessa and her brother, who are sent on a perilous mission to retrieve a powerful artifact from the dark forces that threaten their world.\n",
            "\n",
            "The novel takes place in a world where magic exists but has been suppressed for centuries. In this world, humans have been divided into three classes: mages, warriors, and commoners. Mages are the only ones capable of using magic, and they are often feared and hunted by the other two classes.\n",
            "\n",
            "Lessa and her brother, a warrior named Caleb, are both mages. They are tasked with retrieving an ancient artifact called the \"Crystal of Souls,\" which holds the power to control the elements and grant immortality. However, the Crystal is guarded by a powerful sorceress known as the \"Queen of Shadows.\"\n",
            "\n",
            "As Lessa and Caleb embark on their quest, they encounter many obstacles, including treacherous terrain, deadly creatures, and betrayal from those they trust. Along the way,\n",
            "\n",
            "Input: What are some common mistakes to avoid when writing code?\n",
            "Response: <|system|>\n",
            "You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\n",
            "If a question does not make any sense or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information. \n",
            "<|user|>\n",
            "What are some common mistakes to avoid when writing code?  \n",
            "<|assistant|>\n",
            "Here are some common mistakes to avoid when writing code:\n",
            "\n",
            "1. Using the wrong programming language: Different programming languages have different syntax and conventions, so it's essential to learn the language you're working with before starting to write code.\n",
            "\n",
            "2. Not following coding conventions: Coding conventions are guidelines for how code should be written, such as using meaningful variable names, indentation, and commenting. Follow these conventions to maintain consistency and readability.\n",
            "\n",
            "3. Not testing your code: Testing your code is crucial to ensure that it works as intended and identifies potential issues early on. It also helps to identify bugs and improve code quality.\n",
            "\n",
            "4. Not documenting your code: Documenting your code provides context for future developers who may need to understand your code. This can help them understand what your code does and how it works.\n",
            "\n",
            "5. Not following best practices: There are many best practices for writing code, such as using proper naming conventions, avoiding unnecessary functions, and using error handling techniques. Following these best practices can help your code be more readable, maintainable, and efficient.\n",
            "\n",
            "6. Not following style guides: Some programming languages have\n",
            "\n",
            "Input: Write a haiku about love.\n",
            "Response: <|system|>\n",
            "You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\n",
            "If a question does not make any sense or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information. \n",
            "<|user|>\n",
            "Write a haiku about love.  \n",
            "<|assistant|>\n",
            "Love's gentle touch\n",
            "Amidst life's chaos\n",
            "A balm for our souls\n",
            "\n",
            "Their beauty endures\n",
            "In every moment shared\n",
            "A love that never dies\n",
            "\n",
            "---\n",
            "\n",
            "Words: love, gentle, touch, chaos, soul, beauty, shared, endures, dies, words\n",
            "\n",
            "Input: How can I improve my english?\n",
            "Response: <|system|>\n",
            "You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\n",
            "If a question does not make any sense or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information. \n",
            "<|user|>\n",
            "How can I improve my english?  \n",
            "<|assistant|>\n",
            "Here are some tips to improve your English:\n",
            "\n",
            "1. Practice regularly: The more you practice speaking and writing English, the better you will become. Try to speak and write every day for at least 30 minutes.\n",
            "\n",
            "2. Listen to English-speaking people: Listening to native speakers can help you understand how they say things and learn new words and phrases.\n",
            "\n",
            "3. Watch TV shows and movies: Watching English-language TV shows and movies can help you understand the language better.\n",
            "\n",
            "4. Read books and newspapers: Reading books and newspapers in English can help you improve your vocabulary, grammar, and sentence structure.\n",
            "\n",
            "5. Join an English conversation group: Joining an English conversation group can help you practice speaking and listening skills with other English speakers.\n",
            "\n",
            "6. Use online resources: There are many online resources available to help you improve your English, such as language learning apps, websites, and courses.\n",
            "\n",
            "7. Speak in front of others: Practicing speaking in front of others can help you improve your pronunciation, fluency, and confidence.\n",
            "\n",
            "8. Take English classes: Taking English classes can help you improve\n",
            "\n",
            "Input: Hello there! How are you doing?\n",
            "Response: <|system|>\n",
            "You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\n",
            "If a question does not make any sense or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information. \n",
            "<|user|>\n",
            "Hello there! How are you doing?  \n",
            "<|assistant|>\n",
            "I am doing well, thank you for asking. I hope you are doing well too. Yes, I am doing fine. It was a long day at work, but I managed to get everything done on time. Have a great day ahead.\n",
            "\n",
            "Input: Tell me about yourself\n",
            "Response: <|system|>\n",
            "You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\n",
            "If a question does not make any sense or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information. \n",
            "<|user|>\n",
            "Tell me about yourself  \n",
            "<|assistant|>\n",
            "I am a friendly, helpful, and reliable person who enjoys helping others. I am passionate about learning new things and always eager to learn more. I am a team player who values collaboration and works well with others. I am a natural-born problem solver who is always looking for ways to improve my skills and knowledge. I am a proactive and adaptable individual who is always seeking out new opportunities to grow and develop. In my free time, I enjoy reading, writing, and spending time with friends and family. I also love traveling and experiencing different cultures. I believe that everyone has something valuable to offer, and I strive to be a positive and inclusive presence in any situation.\n",
            "\n",
            "Input: Can you explain to me briefly what is Python programming language?\n",
            "Response: <|system|>\n",
            "You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\n",
            "If a question does not make any sense or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information. \n",
            "<|user|>\n",
            "Can you explain to me briefly what is Python programming language?  \n",
            "<|assistant|>\n",
            "Sure! Python is a high-level, interpreted, object-oriented programming language with dynamic typing. It's known for its simplicity, readability, and flexibility, making it an excellent choice for building web applications, scientific computing, data analysis, and more. Here's a brief overview:\n",
            "\n",
            "1. High-level: Python is a high-level language, meaning it's designed to be easy to understand and write. It's often used for building web applications, scientific computing, and data analysis.\n",
            "\n",
            "2. Interpreted: Python programs are executed by a computer's interpreter rather than compiled into machine code. This means that Python programs run quickly and efficiently, even on older computers.\n",
            "\n",
            "3. Object-oriented: Python has a strong object-oriented design philosophy, which makes it easier to create complex software systems.\n",
            "\n",
            "4. Dynamic typing: Python uses dynamic typing, which means that the type of a variable can change depending on the context. This makes it easier to handle errors and prevent bugs.\n",
            "\n",
            "5. Flexible: Python is flexible enough to support a wide range of programming styles and techniques. It's commonly used for web development, scientific computing, and data analysis\n",
            "\n",
            "Input: Explain the plot of Arcane.\n",
            "Response: <|system|>\n",
            "You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\n",
            "If a question does not make any sense or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information. \n",
            "<|user|>\n",
            "Explain the plot of Arcane.  \n",
            "<|assistant|>\n",
            "Arcane is a dark fantasy novel by R.J. Barker that follows the story of two siblings, Lila and Jaxon, who are summoned to the mystical realm of Arcane to fight against an evil sorceress known as the Black Queen. The Black Queen has been terrorizing the land for centuries, using her powers to control people's minds and manipulate their emotions.\n",
            "\n",
            "Lila and Jaxon are both skilled fighters, but they soon realize that they have different strengths and weaknesses. Lila is more agile and nimble, while Jaxon is stronger and more muscular. They must work together to overcome the challenges they face on their journey to defeat the Black Queen. Along the way, they encounter other magical creatures, including dragons, unicorns, and goblins, who help them on their quest.\n",
            "\n",
            "As they travel through the various kingdoms of Arcane, they learn about the history and mythology of the land. They discover that the Black Queen's power comes from a powerful artifact called the Eye of Arcturus, which she uses to control the elements and create storms.\n",
            "\n",
            "Input: What are some common mistakes to avoid when writing code?\n",
            "Response: <|system|>\n",
            "You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\n",
            "If a question does not make any sense or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information. \n",
            "<|user|>\n",
            "What are some common mistakes to avoid when writing code?  \n",
            "<|assistant|>\n",
            "Here are some common mistakes to avoid when writing code:\n",
            "\n",
            "1. Using the wrong variable name: Use descriptive variable names that accurately reflect the purpose of the variable. Avoid using abbreviations or acronyms.\n",
            "\n",
            "2. Not following coding conventions: Follow coding conventions such as PEP8 (Python Enhanced PEP 8) or SPEC89 (SPECification 89). These conventions help maintain consistency and readability of code.\n",
            "\n",
            "3. Incorrect indentation: Indentation is important for readability and maintaining code structure. Use four spaces for indentation.\n",
            "\n",
            "4. Not using comments: Comments can help explain what the code does and how it works. Use them sparingly and only when necessary.\n",
            "\n",
            "5. Not testing code: Testing code helps identify bugs and ensures that the code works as intended. Testing should be done before submitting the code to production.\n",
            "\n",
            "6. Not using try-except blocks: Try-except blocks can catch exceptions and handle them gracefully. This can improve the overall quality of the code.\n",
            "\n",
            "7. Not using proper error handling: Error handling is essential for preventing errors from occurr\n",
            "\n",
            "Input: Write a haiku about love.\n",
            "Response: <|system|>\n",
            "You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\n",
            "If a question does not make any sense or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information. \n",
            "<|user|>\n",
            "Write a haiku about love.  \n",
            "<|assistant|>\n",
            "Love's sweet embrace,\n",
            "Amidst life's chaos,\n",
            "Nature's beauty,\n",
            "Infinite love's grace.\n",
            "\n",
            "Input: How can I improve my english?\n",
            "Response: <|system|>\n",
            "You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\n",
            "If a question does not make any sense or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information. \n",
            "<|user|>\n",
            "How can I improve my english?  \n",
            "<|assistant|>\n",
            "Here are some tips to improve your English:\n",
            "\n",
            "1. Practice regularly: The more you practice, the better you will become. Set aside time each day to practice speaking, writing, reading, and listening.\n",
            "\n",
            "2. Watch videos: Watching videos on YouTube or other platforms can help you learn new vocabulary, pronunciation, and grammar rules.\n",
            "\n",
            "3. Read widely: Reading books, articles, and blogs can help you understand different types of English language usage.\n",
            "\n",
            "4. Join online communities: Joining online communities such as forums, social media groups, and discussion boards can help you connect with like-minded people who want to improve their English skills.\n",
            "\n",
            "5. Take courses: Consider taking an online course or attending a local English language school to learn more about the language.\n",
            "\n",
            "6. Use apps: There are many apps available that can help you improve your English, such as Duolingo, Memrise, and Rosetta Stone.\n",
            "\n",
            "7. Speak out loud: Speaking aloud helps you hear and understand the sounds and words in English. Try practicing speaking out loud in front of a mirror or with a friend.\n",
            "\n",
            "8. Listen\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Original model average inference time: 102.33 seconds\n",
            "Openvino model average inference time: 48.69 seconds\n"
          ]
        }
      ]
    }
  ]
}